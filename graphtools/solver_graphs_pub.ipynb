{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from itertools import product\n",
    "from classification_refined import classify\n",
    "from processing import *\n",
    "from readfiles import *\n",
    "from decision import filter_summary\n",
    "from subgraphclass import make_solver_summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from classification_refined import *\n",
    "import networkx as nx\n",
    "from readfiles import *\n",
    "from metrics import *\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from readfiles import corresp_label_file\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 84  # number of nodes in the graph\n",
    "tri = int(num * (num + 1) * 0.5)  # we want only the upper diagonal due to symettry of connections\n",
    "edge_names = ['mean_FA', 'mean_strl', 'num_streamlines']\n",
    "mat = np.triu_indices(84)\n",
    "mews = '/home/skapoor/Thesis/gmwcs-solver'\n",
    "metrics = ['balanced_accuracy', 'accuracy', 'f1_weighted', 'roc_auc_ovr_weighted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, feature, edge, solver_node_wts = 'NEOFAC_E', 'num_streamlines', 'fscores','const'\n",
    "val, thresh, max_num_nodes, per = -0.01, 0, 5, 50 # percentage of features we want to preserve intially\n",
    "choice, classifier, refit_metric, feature_selection = 'random', 'SVC', 'balanced_accuracy', 'baseline'\n",
    "baseline_cases, self_loops = set(), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: right now the matrix whole is not scaled, for computing the fscores and correlation coeff it has to be so.\n",
    "y_train, y_test = computed_subjects(), test_subjects()\n",
    "#y_train_l, y_test_l = y_train[target].map({'M': 0, 'F': 1}), y_test[target].map({'M': 0, 'F':1})\n",
    "y_train_l , y_test_l = y_train[target], y_test[target]\n",
    "X_train = generate_combined_matrix(tri, list(y_train.index))  # need to check indices till here then convert to numpy array\n",
    "X_test = generate_test_data(tri, y_test.index)\n",
    "X_train_l, X_test_l = edge_filtering(feature, X_train, X_test)\n",
    "\n",
    "X, y = X_train_l.append(X_test_l), y_train_l.append(y_test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(X.index) == list(y.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=22, shuffle=True)\n",
      "Keeping files: False\n",
      "Selected indices [   3    6    7 ... 3564 3566 3567]\n",
      "Solver runtime for fold 1: 700.7479250431061\n",
      "Best estimator {'C': 1.4588671378559612, 'class_weight': None, 'gamma': 0.008047710790476956, 'kernel': 'rbf'}\n",
      "Keeping files: False\n",
      "Selected indices [   2    3    4 ... 3566 3567 3569]\n",
      "Solver runtime for fold 2: 700.8306376934052\n",
      "Best estimator {'C': 23.448783561507163, 'class_weight': 'balanced', 'gamma': 0.0008331759812852307, 'kernel': 'linear'}\n",
      "Keeping files: False\n",
      "Selected indices [   3    6   11 ... 3561 3564 3566]\n",
      "Solver runtime for fold 3: 700.7656002044678\n",
      "Best estimator {'C': 0.20845791133173502, 'class_weight': 'balanced', 'gamma': 0.0001577727839700561, 'kernel': 'linear'}\n",
      "Keeping files: False\n",
      "Selected indices [   3    6   15 ... 3562 3566 3567]\n",
      "Solver runtime for fold 4: 645.1273715496063\n",
      "Keeping files: True\n",
      "Selected indices [   3    5    6 ... 3566 3567 3568]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_binned = pd.qcut(y, 5, labels=False, retbins=True)[0]\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle =True, random_state = 22)\n",
    "skf.get_n_splits(X, y_binned)\n",
    "\n",
    "print(skf)\n",
    "results_solver = []\n",
    "avg_thresh, self_loops = False, False\n",
    "feature_selection = 'solver'\n",
    "refit_metric = 'balanced_accuracy'\n",
    "nodes = []\n",
    "node_names = []\n",
    "i =0 \n",
    "for train_index, test_index in skf.split(X, y_binned):\n",
    "    i+=1\n",
    "\n",
    "    #print(\"TRAIN:\",len(train_index), train_index, \"TEST:\", len(test_index),test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    med = y_train.median()\n",
    "    y_train_l = pd.qcut(y_train, 5, labels=False, retbins=True)[0]\n",
    "    # we need to pass the non-binned values for effective pearson correlation calc.\n",
    "    # print('The number of training subjects which are to be removed:', sum(y_train_l == 2))\n",
    "    y_train_l = y_train_l[y_train_l != 2]\n",
    "    y_train_l = y_train_l // 3  # binarizing the values by removing the middle quartile\n",
    "    y_test_l = y_test >= med \n",
    "    X_train_l = X_train.loc[y_train_l.index]\n",
    "    assert list(X_train_l.index) == list(y_train_l.index)\n",
    "    \n",
    "    \n",
    "    case = (classifier, target, choice, edge, feature_selection, feature, per, refit_metric, self_loops)\n",
    "    # the baseline needs self loops command but the solver does not!\n",
    "    # drop the diagonal indices from the columns\n",
    "\n",
    "    if feature_selection == 'solver':\n",
    "        if i == 1:\n",
    "            first_run = True\n",
    "        else:\n",
    "            first_run = False\n",
    "        if i< 5:\n",
    "            keep_files = False\n",
    "        else:\n",
    "            keep_files = True\n",
    "        print('Keeping files:', keep_files)\n",
    "        X_train_l, X_test_l, arr, index = transform_array(X_train_l, X_test, y_train_l, per, edge)\n",
    "        assert len(X_train_l) == len(y_train_l)\n",
    "        assert len(X_test_l) == len(y_test_l)\n",
    "        # absolute values of pearson correlation are given accordingly in the transform array function\n",
    "        # index contains the indexes of the top percentile of features\n",
    "        for idx in range(len(arr)):\n",
    "            if idx not in index:\n",
    "                arr.iloc[idx] = 0\n",
    "        start = time.time()\n",
    "        X_train_l, X_test_l, output_graph = solver_pub(X_train_l, X_test_l, y_train_l, arr, \n",
    "                                                   feature, thresh, val, max_num_nodes, node_wts = solver_node_wts, \n",
    "                                                   target = target, edge = edge, \n",
    "                                                   keep_files = keep_files, first_run = first_run)\n",
    "        \n",
    "        end = time.time()\n",
    "        print(f'Solver runtime for fold {i}:', end - start)\n",
    "        nodes.append(list(output_graph.nodes))\n",
    "        node_names.append(output_graph.node_labels)\n",
    "        edge_wts = output_graph.edge_weights\n",
    "        if len(edge_wts) != 0:\n",
    "            train_res, test_res = cross_validation(classifier, X_train_l, y_train_l, X_test_l, y_test_l,\n",
    "                                                   metrics, refit_metric)\n",
    "            # to make the program faster only do this when the solver is actually producing some results\n",
    "            results_solver.append(\n",
    "                [classifier, target, choice, edge, feature_selection, feature, len(edge_wts) * 100 / (tri-84),\n",
    "                 refit_metric, max_num_nodes,\n",
    "                 len(edge_wts), sum([edge > 0 for edge in edge_wts]) * 100 / len(edge_wts)])\n",
    "            results_solver[-1].append(thresh)\n",
    "            for metric in metrics:\n",
    "                results_solver[-1].extend([round(100*train_res[metric],3)])\n",
    "            for metric in metrics:\n",
    "                results_solver[-1].extend([round(100*test_res[metric],3)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.sort(np.concatenate(nodes).flat)\n",
    "\n",
    "d1 = {k-1:v for k,v in corresp_label_file('fs_default.txt').items() if k-1 in np.unique(a)}\n",
    "\n",
    "d2 = {}\n",
    "for k,v in zip(range(len(np.unique(a))), np.unique(a)):\n",
    "    d2[v] = k\n",
    "\n",
    "for i in range(len(a)):\n",
    "    a[i] = d2[a[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(a, bins=len(np.unique(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plt.cm.get_cmap('cool')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,10))\n",
    "counts, bins, patches = ax.hist(a, bins=len(np.unique(a)))\n",
    "for c, p in zip(counts, patches):\n",
    "    plt.setp(p, 'facecolor', cm(c/5))\n",
    "# Set the ticks to be at the edges of the bins.\n",
    "plt.xticks(range(len(np.unique(a))),list(d1.values()), rotation=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameter settings decided on the basis of thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(lin):\n",
    "    \"\"\"\n",
    "    Takes a list of indices and finds the corresponding row and column numbers.\n",
    "    The row and column numbers represent the node number in the LUT.\n",
    "    \n",
    "    Parameters:\n",
    "    lin (int): List of integer indices \n",
    "    \n",
    "    Returns:\n",
    "    d1 (dict): key-value pair of edges and corresponding nodes\n",
    "    \"\"\"\n",
    "    d1 = {}\n",
    "    mat = np.triu_indices(84)\n",
    "    nondiag = list(set(range(3570)).difference(set(diag_flattened_indices(84))))\n",
    "    for idx in lin: \n",
    "        for i in range(len(mat[0])):\n",
    "            if i == idx:\n",
    "                d1[idx] = (mat[0][i], mat[1][i])\n",
    "    return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = BrainGraph('fscores', 'num_streamlines', 'const', 'Gender', 10, -0.01, 0)\n",
    "feature_indices = ip.read_from_file(mews, False)\n",
    "#ip.visualize_graph(mews, False, plotting_options=graph_options('red',10, 'yellow', 2, 1.5),\n",
    "#                   figs=(10,6))\n",
    "G = ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_strls = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = find_indices(feature_indices)\n",
    "ew = G.edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_strls.columns = num_strls.columns - 2*3570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_strls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tups = []\n",
    "nums = []\n",
    "d1 = find_indices(feature_indices)\n",
    "for edge, index in zip(G.edges, feature_indices):\n",
    "    tups.append((edge[0], edge[1], num_strls.loc[:, index].mean()))\n",
    "    nums.append(num_strls.loc[:, index].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_weighted_edges_from(tups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minima = min(nums)\n",
    "maxima = max(nums)\n",
    "norm = matplotlib.colors.Normalize(vmin=minima, vmax=maxima)\n",
    "mapper = cm.ScalarMappable(norm=norm, cmap=plt.get_cmap('Spectral'))\n",
    "color = []\n",
    "for v in nums:\n",
    "    color.append(mapper.to_rgba(v))\n",
    "    \n",
    "    \n",
    "n = len(G.nodes)\n",
    "node_list = list(G.nodes)\n",
    "angle = []\n",
    "angle_dict = {}\n",
    "for i, node in zip(range(n),node_list):\n",
    "    theta = 2.0*np.pi*i/n\n",
    "    angle.append((np.cos(theta),np.sin(theta)))\n",
    "    angle_dict[node] = theta\n",
    "pos = {}\n",
    "for node_i, node in enumerate(node_list):\n",
    "    pos[node] = angle[node_i]\n",
    "\n",
    "labels = {}\n",
    "for node, i in zip(G.nodes, range(len(G.nodes))):\n",
    "    labels[node] = G.node_labels[i]\n",
    "# figsize is intentionally set small to condense the graph\n",
    "fig, ax = plt.subplots(figsize=(11.5,10))\n",
    "margin=0.33\n",
    "fig.subplots_adjust(margin, margin, 1.-margin, 1.-margin)\n",
    "ax.axis('equal')\n",
    "\n",
    "nx.draw(G,pos=pos,with_labels=False, ax=ax, edge_color=color, edge_cmap=mapper.cmap, vmin=minima,\n",
    "                        vmax=maxima, width=[i*20 for i in ew], rotate=60)\n",
    "description = nx.draw_networkx_labels(G,pos,labels=labels, font_size=18)\n",
    "\n",
    "r = fig.canvas.get_renderer()\n",
    "trans = plt.gca().transData.inverted()\n",
    "for node, t in description.items():\n",
    "    bb = t.get_window_extent(renderer=r)\n",
    "    bbdata = bb.transformed(trans)\n",
    "    radius = 1.2+bbdata.width/2.\n",
    "    position = (radius*np.cos(angle_dict[node]),radius* np.sin(angle_dict[node]))\n",
    "    t.set_position(position)\n",
    "    t.set_rotation(angle_dict[node]*360.0/(2.0*np.pi))\n",
    "    t.set_clip_on(False)\n",
    "cbaxes = fig.add_axes([0.001, 0.4, 0.03, 0.2])  # This is the position for the colorbar\n",
    "cb = plt.colorbar(mapper, cax = cbaxes)\n",
    "plt.savefig('outputs/figures/solver_10nodes_strls.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
